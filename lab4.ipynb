{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kg34vjXqk8yB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.6.1 (SDL 2.28.4, Python 3.13.3)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "import pygame\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "class TankEnv:\n",
        "    def __init__(self):\n",
        "        pygame.init()\n",
        "        self.width, self.height = 800, 600\n",
        "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
        "        pygame.display.set_caption(\"Tank Reinforcement Learning Environment\")\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.font = pygame.font.SysFont('Arial', 16)\n",
        "\n",
        "        self.tank_speed = 3\n",
        "        self.tank_size = 20\n",
        "        self.target_radius = 15\n",
        "\n",
        "        # Препятствия\n",
        "        self.obstacles = [\n",
        "            pygame.Rect(300, 200, 200, 50),\n",
        "            pygame.Rect(400, 400, 100, 200),\n",
        "            pygame.Rect(100, 300, 150, 30),\n",
        "            pygame.Rect(600, 100, 50, 300)\n",
        "        ]\n",
        "\n",
        "        # Инициализация танка и цели\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Сброс среды в начальное состояние\"\"\"\n",
        "        self.tank = {\n",
        "            'x': random.randint(50, self.width-50),\n",
        "            'y': random.randint(50, self.height-50),\n",
        "            'angle': random.randint(0, 359),\n",
        "            'health': 100\n",
        "        }\n",
        "\n",
        "        self.target = {\n",
        "            'x': random.randint(50, self.width-50),\n",
        "            'y': random.randint(50, self.height-50)\n",
        "        }\n",
        "\n",
        "        # Проверка, чтобы цель не появлялась в препятствиях\n",
        "        target_rect = pygame.Rect(\n",
        "            self.target['x'] - self.target_radius,\n",
        "            self.target['y'] - self.target_radius,\n",
        "            self.target_radius * 2,\n",
        "            self.target_radius * 2\n",
        "        )\n",
        "\n",
        "        for obs in self.obstacles:\n",
        "            if target_rect.colliderect(obs):\n",
        "                return self.reset()  # Рекурсивно пробуем снова\n",
        "\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"Получение текущего состояния среды\"\"\"\n",
        "        dx = self.target['x'] - self.tank['x']\n",
        "        dy = self.target['y'] - self.tank['y']\n",
        "        distance = math.sqrt(dx**2 + dy**2)\n",
        "        angle_to_target = math.atan2(dy, dx) - math.radians(self.tank['angle'])\n",
        "\n",
        "        # Нормализация угла в диапазон [-π, π]\n",
        "        angle_to_target = (angle_to_target + math.pi) % (2 * math.pi) - math.pi\n",
        "\n",
        "        # Определение расстояний до препятствий в 4 направлениях\n",
        "        obstacle_distances = self._get_obstacle_distances()\n",
        "\n",
        "        return np.array([\n",
        "            self.tank['x'] / self.width,  # Нормированная x-координата танка\n",
        "            self.tank['y'] / self.height,  # Нормированная y-координата танка\n",
        "            self.tank['angle'] / 360,     # Нормированный угол\n",
        "            distance / math.sqrt(self.width**2 + self.height**2),  # Нормированное расстояние\n",
        "            angle_to_target / math.pi,     # Нормированный угол до цели\n",
        "            *obstacle_distances           # Расстояния до препятствий\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def _get_obstacle_distances(self):\n",
        "        \"\"\"Вычисление расстояний до препятствий в 4 направлениях\"\"\"\n",
        "        angles = [0, 90, 180, 270]  # Вперед, вправо, назад, влево\n",
        "        distances = []\n",
        "\n",
        "        for angle in angles:\n",
        "            ray_angle = math.radians(self.tank['angle'] + angle)\n",
        "            dist = self._cast_ray(ray_angle)\n",
        "            distances.append(dist / max(self.width, self.height))  # Нормализация\n",
        "\n",
        "        return distances\n",
        "\n",
        "    def _cast_ray(self, angle):\n",
        "        \"\"\"Бросок луча для определения расстояния до препятствия\"\"\"\n",
        "        step = 5\n",
        "        x, y = self.tank['x'], self.tank['y']\n",
        "\n",
        "        for d in range(0, 500, step):\n",
        "            x += step * math.cos(angle)\n",
        "            y += step * math.sin(angle)\n",
        "\n",
        "            # Проверка выхода за границы\n",
        "            if not (0 <= x <= self.width and 0 <= y <= self.height):\n",
        "                return d\n",
        "\n",
        "            # Проверка столкновения с препятствиями\n",
        "            for obs in self.obstacles:\n",
        "                if obs.collidepoint(x, y):\n",
        "                    return d\n",
        "\n",
        "        return 500  # Максимальное расстояние\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Выполнение действия и возврат нового состояния\"\"\"\n",
        "        reward = -0.1  # Штраф за каждый шаг\n",
        "        done = False\n",
        "        info = {'reached_target': False, 'hit_obstacle': False}\n",
        "\n",
        "        # Обработка действий\n",
        "        if action == 0:  # Вперед\n",
        "            self.tank['x'] += self.tank_speed * math.cos(math.radians(self.tank['angle']))\n",
        "            self.tank['y'] += self.tank_speed * math.sin(math.radians(self.tank['angle']))\n",
        "        elif action == 1:  # Назад\n",
        "            self.tank['x'] -= self.tank_speed * math.cos(math.radians(self.tank['angle']))\n",
        "            self.tank['y'] -= self.tank_speed * math.sin(math.radians(self.tank['angle']))\n",
        "        elif action == 2:  # Влево\n",
        "            self.tank['angle'] = (self.tank['angle'] - 5) % 360\n",
        "        elif action == 3:  # Вправо\n",
        "            self.tank['angle'] = (self.tank['angle'] + 5) % 360\n",
        "\n",
        "        # Проверка границ\n",
        "        self.tank['x'] = np.clip(self.tank['x'], 0, self.width)\n",
        "        self.tank['y'] = np.clip(self.tank['y'], 0, self.height)\n",
        "\n",
        "        # Проверка столкновений с препятствиями\n",
        "        tank_rect = pygame.Rect(\n",
        "            self.tank['x'] - self.tank_size//2,\n",
        "            self.tank['y'] - self.tank_size//2,\n",
        "            self.tank_size,\n",
        "            self.tank_size\n",
        "        )\n",
        "\n",
        "        for obs in self.obstacles:\n",
        "            if tank_rect.colliderect(obs):\n",
        "                reward = -10\n",
        "                done = True\n",
        "                info['hit_obstacle'] = True\n",
        "                break\n",
        "\n",
        "        # Проверка достижения цели\n",
        "        if math.dist((self.tank['x'], self.tank['y']),\n",
        "                    (self.target['x'], self.target['y'])) < self.target_radius + self.tank_size//2:\n",
        "            reward = 100\n",
        "            done = True\n",
        "            info['reached_target'] = True\n",
        "\n",
        "        # Дополнительные награды/штрафы\n",
        "        dx = self.target['x'] - self.tank['x']\n",
        "        dy = self.target['y'] - self.tank['y']\n",
        "        new_dist = math.sqrt(dx**2 + dy**2)\n",
        "\n",
        "        if hasattr(self, 'prev_dist'):\n",
        "            if new_dist < self.prev_dist:\n",
        "                reward += 0.5  # Награда за приближение\n",
        "            else:\n",
        "                reward -= 0.3  # Штраф за удаление\n",
        "        self.prev_dist = new_dist\n",
        "\n",
        "        return self._get_state(), reward, done, info\n",
        "\n",
        "    def render(self, episode=None, reward=None):\n",
        "        \"\"\"Отрисовка текущего состояния среды (исправленная версия)\"\"\"\n",
        "        self.screen.fill((240, 240, 240))\n",
        "\n",
        "        # Отрисовка препятствий\n",
        "        for obs in self.obstacles:\n",
        "            pygame.draw.rect(self.screen, (70, 70, 70), obs)\n",
        "\n",
        "        # Отрисовка цели (исправленный вызов pygame.draw.circle)\n",
        "        pygame.draw.circle(\n",
        "            self.screen,\n",
        "            (255, 50, 50),\n",
        "            (int(self.target['x']), int(self.target['y'])),  # Центр как tuple (x,y)\n",
        "            self.target_radius  # Радиус\n",
        "        )\n",
        "\n",
        "        # Отрисовка танка\n",
        "        tank_color = (50, 150, 255) if self.tank['health'] > 50 else (255, 150, 50)\n",
        "        tank_points = [\n",
        "            (self.tank['x'] + self.tank_size*math.cos(math.radians(self.tank['angle']))),\n",
        "            (self.tank['y'] + self.tank_size*math.sin(math.radians(self.tank['angle']))),\n",
        "            (self.tank['x'] + (self.tank_size//2)*math.cos(math.radians(self.tank['angle']+120))),\n",
        "            (self.tank['y'] + (self.tank_size//2)*math.sin(math.radians(self.tank['angle']+120))),\n",
        "            (self.tank['x'] + (self.tank_size//2)*math.cos(math.radians(self.tank['angle']-120))),\n",
        "            (self.tank['y'] + (self.tank_size//2)*math.sin(math.radians(self.tank['angle']-120)))\n",
        "        ]\n",
        "        pygame.draw.polygon(self.screen, tank_color, tank_points)\n",
        "\n",
        "        # Отрисовка информации\n",
        "        if episode is not None and reward is not None:\n",
        "            info_text = f\"Episode: {episode} | Reward: {reward:.1f} | Angle: {self.tank['angle']}°\"\n",
        "            text_surface = self.font.render(info_text, True, (0, 0, 0))\n",
        "            self.screen.blit(text_surface, (10, 10))\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dFv5uY_Tmsgw"
      },
      "outputs": [],
      "source": [
        "# State (пример для DQN)\n",
        "state_size = 5  # [норм_x, норм_y, норм_угол, норм_расст, норм_угол_к_цели]\n",
        "\n",
        "# Actions\n",
        "action_space = [\n",
        "    \"вперед\", \"назад\", \"влево\", \"вправо\", \"огонь\"\n",
        "]\n",
        "\n",
        "# Reward function (дополнение к коду среды)\n",
        "def calculate_reward(self):\n",
        "    reward = -1  # Штраф за шаг\n",
        "    prev_dist = math.dist(self.prev_pos, (self.target['x'], self.target['y']))\n",
        "    curr_dist = math.dist((self.tank['x'], self.tank['y']),\n",
        "                         (self.target['x'], self.target['y']))\n",
        "\n",
        "    if curr_dist < prev_dist:\n",
        "        reward += 10  # Награда за приближение\n",
        "    else:\n",
        "        reward -= 5   # Штраф за удаление\n",
        "\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZEawu2kmvmx",
        "outputId": "96e2f708-d7e0-43d5-be8d-93f477c4dc4d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.998\n",
        "        self.batch_size = 128\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "        self.loss_history = []\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, input_dim=self.state_size, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(\n",
        "            loss=tf.keras.losses.Huber(),\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.00025),\n",
        "            metrics=['mae']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = np.reshape(state, [1, self.state_size])\n",
        "        return np.argmax(self.model.predict(state, verbose=0)[0])\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = np.array([x[0] for x in minibatch])\n",
        "        actions = np.array([x[1] for x in minibatch])\n",
        "        rewards = np.array([x[2] for x in minibatch])\n",
        "        next_states = np.array([x[3] for x in minibatch])\n",
        "        dones = np.array([x[4] for x in minibatch])\n",
        "\n",
        "        # Double DQN\n",
        "        current_q = self.model.predict(states, verbose=0)\n",
        "        next_q = self.model.predict(next_states, verbose=0)\n",
        "        next_target_q = self.target_model.predict(next_states, verbose=0)\n",
        "\n",
        "        max_actions = np.argmax(next_q, axis=1)\n",
        "        targets = rewards + self.gamma * next_target_q[np.arange(self.batch_size), max_actions] * (1 - dones)\n",
        "\n",
        "        current_q[np.arange(self.batch_size), actions] = targets\n",
        "\n",
        "        history = self.model.fit(\n",
        "            states,\n",
        "            current_q,\n",
        "            batch_size=self.batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "        self.loss_history.append(history.history['loss'][0])\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# Инициализация и обучение (20 эпизодов)\n",
        "env = TankEnv()\n",
        "agent = DQNAgent(state_size=9, action_size=5)\n",
        "episodes = 10\n",
        "target_update_freq = 2  # Обновлять target network каждые 10 эпизодов\n",
        "reward_history = []\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    while True:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            reward_history.append(total_reward)\n",
        "            if e % target_update_freq == 0:\n",
        "                agent.update_target_model()\n",
        "\n",
        "            print(f\"Эпизод: {e+1:2d}/{episodes}, \"\n",
        "                  f\"Награда: {total_reward:7.2f}, \"\n",
        "                  f\"Epsilon: {agent.epsilon:.3f}, \"\n",
        "                  f\"Loss: {np.mean(agent.loss_history[-10:] if agent.loss_history else 0):.4f}\")\n",
        "            break\n",
        "\n",
        "    agent.replay()\n",
        "\n",
        "# Вывод итоговой статистики\n",
        "print(\"\\nОбучение завершено!\")\n",
        "print(f\"Средняя награда: {np.mean(reward_history):.2f}\")\n",
        "print(f\"Максимальная награда: {max(reward_history):.2f}\")\n",
        "print(f\"Минимальная награда: {min(reward_history):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXu1DuiY0CwS"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from time import sleep\n",
        "# import numpy as np\n",
        "# import pygame\n",
        "# import random\n",
        "\n",
        "# def test_and_analyze(env, agent, reward_history):\n",
        "#     \"\"\"Основная функция для тестирования и анализа\"\"\"\n",
        "\n",
        "#     # 1. Вложенная функция тестирования агента\n",
        "#     def _test_agent(render=True):\n",
        "#         \"\"\"Тестирование агента (внутренняя функция)\"\"\"\n",
        "#         print(\"\\nТестирование обученного агента (5 эпизодов):\")\n",
        "#         test_rewards = []\n",
        "#         original_epsilon = agent.epsilon\n",
        "#         agent.epsilon = 0.01\n",
        "\n",
        "#         for test_ep in range(5):\n",
        "#             state = env.reset()\n",
        "#             total_reward = 0\n",
        "#             done = False\n",
        "\n",
        "#             while not done:\n",
        "#                 if render:\n",
        "#                     env.render(episode=test_ep+1, reward=total_reward)\n",
        "#                     sleep(0.03)\n",
        "\n",
        "#                 action = agent.act(state)\n",
        "#                 next_state, reward, done, _ = env.step(action)\n",
        "#                 state = next_state\n",
        "#                 total_reward += reward\n",
        "\n",
        "#             test_rewards.append(total_reward)\n",
        "#             print(f\"Тест {test_ep+1}: Награда = {total_reward:.1f}\")\n",
        "\n",
        "#         agent.epsilon = original_epsilon\n",
        "#         return test_rewards\n",
        "\n",
        "#     # 2. Визуализация процесса обучения\n",
        "#     def _plot_learning():\n",
        "#         plt.figure(figsize=(10, 5))\n",
        "#         plt.plot(reward_history)\n",
        "#         plt.title(\"История наград во время обучения\")\n",
        "#         plt.xlabel(\"Эпизод\")\n",
        "#         plt.ylabel(\"Награда\")\n",
        "#         plt.grid(True)\n",
        "#         plt.show()\n",
        "\n",
        "#     # 3. Тестирование разных начальных условий\n",
        "#     def _test_various_conditions():\n",
        "#         print(\"\\nТестирование разных начальных условий:\")\n",
        "#         for i in range(3):\n",
        "#             env.tank = {\n",
        "#                 'x': random.randint(50, env.width-50),\n",
        "#                 'y': random.randint(50, env.height-50),\n",
        "#                 'angle': random.randint(0, 359),\n",
        "#                 'health': 100\n",
        "#             }\n",
        "#             env.target = {\n",
        "#                 'x': random.randint(50, env.width-50),\n",
        "#                 'y': random.randint(50, env.height-50)\n",
        "#             }\n",
        "\n",
        "#             state = env._get_state()\n",
        "#             total_reward = 0\n",
        "#             done = False\n",
        "\n",
        "#             while not done:\n",
        "#                 env.render(episode=i+1, reward=total_reward)\n",
        "#                 sleep(0.05)\n",
        "\n",
        "#                 action = agent.act(state)\n",
        "#                 next_state, reward, done, _ = env.step(action)\n",
        "#                 state = next_state\n",
        "#                 total_reward += reward\n",
        "\n",
        "#             print(f\"Конфигурация {i+1}: Награда = {total_reward:.1f}\")\n",
        "\n",
        "#     # Запуск всех тестов\n",
        "#     print(\"\\n=== Начало тестирования ===\")\n",
        "#     test_rewards = _test_agent()\n",
        "#     _plot_learning()\n",
        "#     _test_various_conditions()\n",
        "\n",
        "#     # Вывод статистики\n",
        "#     print(\"\\n=== Итоговая статистика ===\")\n",
        "#     print(f\"Средняя награда при обучении: {np.mean(reward_history):.2f}\")\n",
        "#     print(f\"Средняя награда при тестировании: {np.mean(test_rewards):.2f}\")\n",
        "#     print(f\"Финальный epsilon: {agent.epsilon:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
